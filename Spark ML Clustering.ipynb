{"cells":[{"cell_type":"markdown","source":["#Spark ML Clustering\n\nWe use Spark ML to perform clustering. The dataset we are using has acceleration recording from wearable devices in x, y and z coordinates. The datapoints are divided into different classes which categorize what the person was doing while the acceleromoter was recording the values (e.g. walking, brushing teeth, pouring water, etc.)"],"metadata":{}},{"cell_type":"code","source":["# delete files from previous runs\n!rm -f hmp.parquet*\n\n# download the file containing the data in PARQUET format\n!wget -P /tmp https://github.com/IBM/coursera/raw/master/hmp.parquet"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["display(dbutils.fs.ls(\"file:/tmp/hmp.parquet\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>file:/tmp/hmp.parquet</td><td>hmp.parquet</td><td>932997</td></tr></tbody></table></div>"]}}],"execution_count":3},{"cell_type":"code","source":["# create a dataframe out of it\ndf = spark.read.parquet('file:/tmp/hmp.parquet')\n\n# register a corresponding query table\ndf.createOrReplaceTempView('df')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["df.show(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+---+---+--------------------+-----------+\n  x|  y|  z|              source|      class|\n+---+---+---+--------------------+-----------+\n 22| 49| 35|Accelerometer-201...|Brush_teeth|\n 22| 49| 35|Accelerometer-201...|Brush_teeth|\n 22| 52| 35|Accelerometer-201...|Brush_teeth|\n 22| 52| 35|Accelerometer-201...|Brush_teeth|\n 21| 52| 34|Accelerometer-201...|Brush_teeth|\n 22| 51| 34|Accelerometer-201...|Brush_teeth|\n 20| 50| 35|Accelerometer-201...|Brush_teeth|\n 22| 52| 34|Accelerometer-201...|Brush_teeth|\n 22| 50| 34|Accelerometer-201...|Brush_teeth|\n 22| 51| 35|Accelerometer-201...|Brush_teeth|\n+---+---+---+--------------------+-----------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["# let's look at the different classes in our dataset\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\ndisplay(sqlContext.sql(\"select class, count(*) as `count` from df group by class\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>class</th><th>count</th></tr></thead><tbody><tr><td>Use_telephone</td><td>15225</td></tr><tr><td>Standup_chair</td><td>25417</td></tr><tr><td>Eat_meat</td><td>31236</td></tr><tr><td>Getup_bed</td><td>45801</td></tr><tr><td>Drink_glass</td><td>42792</td></tr><tr><td>Pour_water</td><td>41673</td></tr><tr><td>Comb_hair</td><td>23504</td></tr><tr><td>Walk</td><td>92254</td></tr><tr><td>Climb_stairs</td><td>40258</td></tr><tr><td>Sitdown_chair</td><td>25036</td></tr><tr><td>Liedown_bed</td><td>11446</td></tr><tr><td>Descend_stairs</td><td>15375</td></tr><tr><td>Brush_teeth</td><td>29829</td></tr><tr><td>Eat_soup</td><td>6683</td></tr></tbody></table></div>"]}}],"execution_count":6},{"cell_type":"code","source":["# number of distinct classes in our dataset\ndisplay(sqlContext.sql(\"select count(distinct class) from df\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(DISTINCT class)</th></tr></thead><tbody><tr><td>14</td></tr></tbody></table></div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["Let's do some preprocessing to data"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer\n\nindexer = StringIndexer(inputCol=\"class\", outputCol=\"classIndex\")\nindexed = indexer.fit(df).transform(df)\nindexed.show(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+---+---+--------------------+-----------+----------+\n  x|  y|  z|              source|      class|classIndex|\n+---+---+---+--------------------+-----------+----------+\n 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|\n 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|\n 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|\n 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|\n 21| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|\n 22| 51| 34|Accelerometer-201...|Brush_teeth|       6.0|\n 20| 50| 35|Accelerometer-201...|Brush_teeth|       6.0|\n 22| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|\n 22| 50| 34|Accelerometer-201...|Brush_teeth|       6.0|\n 22| 51| 35|Accelerometer-201...|Brush_teeth|       6.0|\n+---+---+---+--------------------+-----------+----------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["from pyspark.ml.feature import OneHotEncoder\n\nencoder = OneHotEncoder(inputCol=\"classIndex\", outputCol=\"categoryVec\")\nencoded = encoder.transform(indexed)\nencoded.show(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+---+---+--------------------+-----------+----------+--------------+\n  x|  y|  z|              source|      class|classIndex|   categoryVec|\n+---+---+---+--------------------+-----------+----------+--------------+\n 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|\n 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|\n 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|\n 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|\n 21| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|\n 22| 51| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|\n 20| 50| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|\n 22| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|\n 22| 50| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|\n 22| 51| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|\n+---+---+---+--------------------+-----------+----------+--------------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.linalg import Vectors\n\nvectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"], outputCol=\"features\")\nvecFeatures = vectorAssembler.transform(encoded)\nvecFeatures.show(10)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+---+---+--------------------+-----------+----------+--------------+----------------+\n  x|  y|  z|              source|      class|classIndex|   categoryVec|        features|\n+---+---+---+--------------------+-----------+----------+--------------+----------------+\n 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,49.0,35.0]|\n 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,49.0,35.0]|\n 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,35.0]|\n 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,35.0]|\n 21| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[21.0,52.0,34.0]|\n 22| 51| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,51.0,34.0]|\n 20| 50| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[20.0,50.0,35.0]|\n 22| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,34.0]|\n 22| 50| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,50.0,34.0]|\n 22| 51| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,51.0,35.0]|\n+---+---+---+--------------------+-----------+----------+--------------+----------------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["from pyspark.ml.feature import Normalizer\n\nnormalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)\nnormFeatures = normalizer.transform(vecFeatures)\nnormFeatures.show(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+\n  x|  y|  z|              source|      class|classIndex|   categoryVec|        features|       features_norm|\n+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+\n 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|\n 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|\n 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|\n 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|\n 21| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[21.0,52.0,34.0]|[0.19626168224299...|\n 22| 51| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,51.0,34.0]|[0.20560747663551...|\n 20| 50| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[20.0,50.0,35.0]|[0.19047619047619...|\n 22| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,34.0]|[0.20370370370370...|\n 22| 50| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,50.0,34.0]|[0.20754716981132...|\n 22| 51| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,51.0,35.0]|[0.20370370370370...|\n+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["Put all the feature engineering steps into a pipeline"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n\npipeline = Pipeline(stages=[indexer, encoder, vectorAssembler, normalizer])\nmodel = pipeline.fit(df)\nprediction = model.transform(df)\nprediction.show(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+\n  x|  y|  z|              source|      class|classIndex|   categoryVec|        features|       features_norm|\n+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+\n 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|\n 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|\n 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|\n 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|\n 21| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[21.0,52.0,34.0]|[0.19626168224299...|\n 22| 51| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,51.0,34.0]|[0.20560747663551...|\n 20| 50| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[20.0,50.0,35.0]|[0.19047619047619...|\n 22| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,34.0]|[0.20370370370370...|\n 22| 50| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,50.0,34.0]|[0.20754716981132...|\n 22| 51| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,51.0,35.0]|[0.20370370370370...|\n+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["Create a new pipeline for Kmeans clustering. \n\nAlso, assuming that we did not have prior knowledge of number of classes, we can use our pipeline to build different variations of our kmeans model and evaluate each one to find the best fit.\n\nWe will evaulaute the clustering models using Silhouette analysis. Silhouette analysis can be used to study the separation distance between the resulting clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters. This measure has a range of [-1, 1]."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.clustering import KMeans\nfrom pyspark.ml.evaluation import ClusteringEvaluator\n\nk_values = [6, 8, 10, 12, 14, 16, 20]\nsilhouette_values = []\nfor k in k_values:\n  \n  kmeans = KMeans(featuresCol=\"features\").setK(k).setSeed(1)\n  pipeline = Pipeline(stages=[vectorAssembler, kmeans])\n  model = pipeline.fit(df)\n  predictions = model.transform(df)\n\n  evaluator = ClusteringEvaluator()\n  silhouette_values.append(evaluator.evaluate(predictions))\n  \n  print(\"With k = \" + str(k) + \": Silhouette with squared euclidean distance = \" + str(silhouette_values[-1]))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">With k = 6: Silhouette with squared euclidean distance = 0.592463658820136\nWith k = 8: Silhouette with squared euclidean distance = 0.46686489256383346\nWith k = 10: Silhouette with squared euclidean distance = 0.47370428136987536\nWith k = 12: Silhouette with squared euclidean distance = 0.40964155503229643\nWith k = 14: Silhouette with squared euclidean distance = 0.41244594513295846\nWith k = 16: Silhouette with squared euclidean distance = 0.39594610810727193\nWith k = 20: Silhouette with squared euclidean distance = 0.3445366343500456\n</div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["It appears from our results that as we increase the number of clusters k, our silhouette scores go down. A reason for this could be that a lot of the classes can be very similar in the type of x,y,z accelerations they produce. For example, 'standup_chair' and 'getup_bed' or 'sitdown_chair' and 'liedown_bed' could be very similar. Another example, is 'walk', 'climb_strairs' and 'descend_stairs' that can generate very simialr recordings since all three activities involve swinging the arms back and forward (assuming the wearable device is on the wrist). So in reality we may only have 6-10 highly distinct classes of activities. \n\nWe achieved silhouette score of 0.4124 for k=14. Let's try normalizing the features to see if helps."],"metadata":{}},{"cell_type":"code","source":["\nkmeans = KMeans(featuresCol=\"features\").setK(14).setSeed(1)\npipeline = Pipeline(stages=[vectorAssembler, normalizer, kmeans])\nmodel = pipeline.fit(df)\npredictions = model.transform(df)\n\nevaluator = ClusteringEvaluator()\nsilhouette = evaluator.evaluate(predictions)\nprint(\"Kmeans with k = \" + str(14) + \": Silhouette with squared euclidean distance = \" + str(silhouette))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">With k = 14: Silhouette with squared euclidean distance = 0.41244594513295846\n</div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["Normalizing didn't make much differnce.\n\nSometimes, inflating the dataset helps, here we multiply x by 10, letâ€™s see if the performance inceases."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col\ndf_denormalized = df.select([col('*'),(col('x')*10)]).drop('x').withColumnRenamed('(x * 10)','x')\ndf_denormalized.show(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+---+--------------------+-----------+---+\n  y|  z|              source|      class|  x|\n+---+---+--------------------+-----------+---+\n 49| 35|Accelerometer-201...|Brush_teeth|220|\n 49| 35|Accelerometer-201...|Brush_teeth|220|\n 52| 35|Accelerometer-201...|Brush_teeth|220|\n 52| 35|Accelerometer-201...|Brush_teeth|220|\n 52| 34|Accelerometer-201...|Brush_teeth|210|\n 51| 34|Accelerometer-201...|Brush_teeth|220|\n 50| 35|Accelerometer-201...|Brush_teeth|200|\n 52| 34|Accelerometer-201...|Brush_teeth|220|\n 50| 34|Accelerometer-201...|Brush_teeth|220|\n 51| 35|Accelerometer-201...|Brush_teeth|220|\n+---+---+--------------------+-----------+---+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":["kmeans = KMeans(featuresCol=\"features\").setK(14).setSeed(1)\npipeline = Pipeline(stages=[vectorAssembler, kmeans])\nmodel = pipeline.fit(df_denormalized)\npredictions = model.transform(df_denormalized)\n\nevaluator = ClusteringEvaluator()\nsilhouette = evaluator.evaluate(predictions)\nprint(\"Kmeans with k = \" + str(14) + \": Silhouette with squared euclidean distance = \" + str(silhouette))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">With k = 14: Silhouette with squared euclidean distance = 0.5709023393004293\n</div>"]}}],"execution_count":21},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":22}],"metadata":{"name":"Spark ML Clustering","notebookId":2993804106246670},"nbformat":4,"nbformat_minor":0}
