{"cells":[{"cell_type":"markdown","source":["#Spark ML Clustering\n\nWe use Spark ML to perform clustering. The dataset we are using has acceleration recording from wearable devices in x, y and z coordinates. The datapoints are divided into different classes which categorize what the person was doing while the acceleromoter was recording the values (e.g. walking, brushing teeth, pouring water, etc.)"],"metadata":{}},{"cell_type":"code","source":["# delete files from previous runs\n!rm -f hmp.parquet*\n\n# download the file containing the data in PARQUET format\n!wget -P /tmp https://github.com/IBM/coursera/raw/master/hmp.parquet"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["display(dbutils.fs.ls(\"file:/tmp/hmp.parquet\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>file:/tmp/hmp.parquet</td><td>hmp.parquet</td><td>932997</td></tr></tbody></table></div>"]}}],"execution_count":3},{"cell_type":"code","source":["# create a dataframe out of it\ndf = spark.read.parquet('file:/tmp/hmp.parquet')\n\n# register a corresponding query table\ndf.createOrReplaceTempView('df')\ndisplay(df.limit(10))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>x</th><th>y</th><th>z</th><th>source</th><th>class</th></tr></thead><tbody><tr><td>22</td><td>49</td><td>35</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td></tr><tr><td>22</td><td>49</td><td>35</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td></tr><tr><td>22</td><td>52</td><td>35</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td></tr><tr><td>22</td><td>52</td><td>35</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td></tr><tr><td>21</td><td>52</td><td>34</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td></tr><tr><td>22</td><td>51</td><td>34</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td></tr><tr><td>20</td><td>50</td><td>35</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td></tr><tr><td>22</td><td>52</td><td>34</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td></tr><tr><td>22</td><td>50</td><td>34</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td></tr><tr><td>22</td><td>51</td><td>35</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td></tr></tbody></table></div>"]}}],"execution_count":4},{"cell_type":"code","source":["# let's look at the different classes in our dataset\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\ndisplay(sqlContext.sql(\"select class, count(*) as `count` from df group by class\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>class</th><th>count</th></tr></thead><tbody><tr><td>Use_telephone</td><td>15225</td></tr><tr><td>Standup_chair</td><td>25417</td></tr><tr><td>Eat_meat</td><td>31236</td></tr><tr><td>Getup_bed</td><td>45801</td></tr><tr><td>Drink_glass</td><td>42792</td></tr><tr><td>Pour_water</td><td>41673</td></tr><tr><td>Comb_hair</td><td>23504</td></tr><tr><td>Walk</td><td>92254</td></tr><tr><td>Climb_stairs</td><td>40258</td></tr><tr><td>Sitdown_chair</td><td>25036</td></tr><tr><td>Liedown_bed</td><td>11446</td></tr><tr><td>Descend_stairs</td><td>15375</td></tr><tr><td>Brush_teeth</td><td>29829</td></tr><tr><td>Eat_soup</td><td>6683</td></tr></tbody></table></div>"]}}],"execution_count":5},{"cell_type":"code","source":["# number of distinct classes in our dataset\ndisplay(sqlContext.sql(\"select count(distinct class) from df\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(DISTINCT class)</th></tr></thead><tbody><tr><td>14</td></tr></tbody></table></div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["Let's do some preprocessing to data"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.linalg import Vectors\n\nvectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"], outputCol=\"features\")\nvecFeatures = vectorAssembler.transform(df)\ndisplay(vecFeatures.limit(10))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>x</th><th>y</th><th>z</th><th>source</th><th>class</th><th>features</th></tr></thead><tbody><tr><td>22</td><td>49</td><td>35</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td><td>List(1, 3, List(), List(22.0, 49.0, 35.0))</td></tr><tr><td>22</td><td>49</td><td>35</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td><td>List(1, 3, List(), List(22.0, 49.0, 35.0))</td></tr><tr><td>22</td><td>52</td><td>35</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td><td>List(1, 3, List(), List(22.0, 52.0, 35.0))</td></tr><tr><td>22</td><td>52</td><td>35</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td><td>List(1, 3, List(), List(22.0, 52.0, 35.0))</td></tr><tr><td>21</td><td>52</td><td>34</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td><td>List(1, 3, List(), List(21.0, 52.0, 34.0))</td></tr><tr><td>22</td><td>51</td><td>34</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td><td>List(1, 3, List(), List(22.0, 51.0, 34.0))</td></tr><tr><td>20</td><td>50</td><td>35</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td><td>List(1, 3, List(), List(20.0, 50.0, 35.0))</td></tr><tr><td>22</td><td>52</td><td>34</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td><td>List(1, 3, List(), List(22.0, 52.0, 34.0))</td></tr><tr><td>22</td><td>50</td><td>34</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td><td>List(1, 3, List(), List(22.0, 50.0, 34.0))</td></tr><tr><td>22</td><td>51</td><td>35</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td><td>List(1, 3, List(), List(22.0, 51.0, 35.0))</td></tr></tbody></table></div>"]}}],"execution_count":8},{"cell_type":"code","source":["from pyspark.ml.feature import Normalizer\n\nnormalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)\nnormFeatures = normalizer.transform(vecFeatures)\ndisplay(normFeatures.limit(10))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>x</th><th>y</th><th>z</th><th>source</th><th>class</th><th>features</th><th>features_norm</th></tr></thead><tbody><tr><td>22</td><td>49</td><td>35</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td><td>List(1, 3, List(), List(22.0, 49.0, 35.0))</td><td>List(1, 3, List(), List(0.20754716981132076, 0.46226415094339623, 0.330188679245283))</td></tr><tr><td>22</td><td>49</td><td>35</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td><td>List(1, 3, List(), List(22.0, 49.0, 35.0))</td><td>List(1, 3, List(), List(0.20754716981132076, 0.46226415094339623, 0.330188679245283))</td></tr><tr><td>22</td><td>52</td><td>35</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td><td>List(1, 3, List(), List(22.0, 52.0, 35.0))</td><td>List(1, 3, List(), List(0.2018348623853211, 0.47706422018348627, 0.3211009174311927))</td></tr><tr><td>22</td><td>52</td><td>35</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td><td>List(1, 3, List(), List(22.0, 52.0, 35.0))</td><td>List(1, 3, List(), List(0.2018348623853211, 0.47706422018348627, 0.3211009174311927))</td></tr><tr><td>21</td><td>52</td><td>34</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td><td>List(1, 3, List(), List(21.0, 52.0, 34.0))</td><td>List(1, 3, List(), List(0.19626168224299065, 0.48598130841121495, 0.3177570093457944))</td></tr><tr><td>22</td><td>51</td><td>34</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td><td>List(1, 3, List(), List(22.0, 51.0, 34.0))</td><td>List(1, 3, List(), List(0.205607476635514, 0.4766355140186916, 0.3177570093457944))</td></tr><tr><td>20</td><td>50</td><td>35</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td><td>List(1, 3, List(), List(20.0, 50.0, 35.0))</td><td>List(1, 3, List(), List(0.19047619047619047, 0.47619047619047616, 0.3333333333333333))</td></tr><tr><td>22</td><td>52</td><td>34</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td><td>List(1, 3, List(), List(22.0, 52.0, 34.0))</td><td>List(1, 3, List(), List(0.2037037037037037, 0.48148148148148145, 0.3148148148148148))</td></tr><tr><td>22</td><td>50</td><td>34</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td><td>List(1, 3, List(), List(22.0, 50.0, 34.0))</td><td>List(1, 3, List(), List(0.20754716981132076, 0.4716981132075472, 0.32075471698113206))</td></tr><tr><td>22</td><td>51</td><td>35</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td><td>List(1, 3, List(), List(22.0, 51.0, 35.0))</td><td>List(1, 3, List(), List(0.2037037037037037, 0.4722222222222222, 0.32407407407407407))</td></tr></tbody></table></div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["Create a pipeline for Kmeans clustering. \n\nAlso, assuming that we did not have prior knowledge of number of classes, we can use our pipeline to build different variations of our kmeans model and evaluate each one to find the best fit.\n\nWe will evaulaute the clustering models using Silhouette analysis. Silhouette analysis can be used to study the separation distance between the resulting clusters. This measure has a range of [-1, 1]."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.clustering import KMeans\nfrom pyspark.ml.evaluation import ClusteringEvaluator\n\nk_values = [6, 8, 10, 12, 14, 16, 20]\nsilhouette_values = []\nfor k in k_values:\n  \n  kmeans = KMeans(featuresCol=\"features\").setK(k).setSeed(1)\n  pipeline = Pipeline(stages=[vectorAssembler, kmeans])\n  model = pipeline.fit(df)\n  predictions = model.transform(df)\n\n  evaluator = ClusteringEvaluator()\n  silhouette_values.append(evaluator.evaluate(predictions))\n  \n  print(\"With k = \" + str(k) + \": Silhouette with squared euclidean distance = \" + str(silhouette_values[-1]))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">With k = 6: Silhouette with squared euclidean distance = 0.592463658820136\nWith k = 8: Silhouette with squared euclidean distance = 0.46686489256383346\nWith k = 10: Silhouette with squared euclidean distance = 0.47370428136987536\nWith k = 12: Silhouette with squared euclidean distance = 0.40964155503229643\nWith k = 14: Silhouette with squared euclidean distance = 0.41244594513295846\nWith k = 16: Silhouette with squared euclidean distance = 0.39594610810727193\nWith k = 20: Silhouette with squared euclidean distance = 0.3445366343500456\n</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["It appears from our results that as we increase the number of clusters k, our silhouette scores go down. \n\nA reason for this could be that a lot of the classes can be very similar in the type of x,y,z accelerations they produce. For example, 'standup_chair' and 'getup_bed' or 'sitdown_chair' and 'liedown_bed' could be very similar. Also, classes 'walk', 'climb_strairs' and 'descend_stairs' can generate very simialr recordings since all three activities involve swinging the arms back and forward (assuming the wearable device is on the wrist). So in reality we may only have 6-10 highly distinct classes of activities. \n\nWe achieved silhouette score of 0.4124 for k=14. Let's try normalizing the features to see if helps."],"metadata":{}},{"cell_type":"code","source":["kmeans = KMeans(featuresCol=\"features\").setK(14).setSeed(1)\npipeline = Pipeline(stages=[vectorAssembler, normalizer, kmeans])\nmodel = pipeline.fit(df)\npredictions = model.transform(df)\n\nevaluator = ClusteringEvaluator()\nsilhouette = evaluator.evaluate(predictions)\nprint(\"Kmeans with k = \" + str(14) + \": Silhouette with squared euclidean distance = \" + str(silhouette))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Kmeans with k = 14: Silhouette with squared euclidean distance = 0.41244594513295846\n</div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["Normalizing didn't make much differnce.\n\nSometimes, inflating the dataset helps, here we multiply x by 10, letâ€™s see if the performance inceases."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col\ndf_denormalized = df.select([col('*'),(col('x')*10)]).drop('x').withColumnRenamed('(x * 10)','x')\ndisplay(df_denormalized.limit(10))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>y</th><th>z</th><th>source</th><th>class</th><th>x</th></tr></thead><tbody><tr><td>49</td><td>35</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td><td>220</td></tr><tr><td>49</td><td>35</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td><td>220</td></tr><tr><td>52</td><td>35</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td><td>220</td></tr><tr><td>52</td><td>35</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td><td>220</td></tr><tr><td>52</td><td>34</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td><td>210</td></tr><tr><td>51</td><td>34</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td><td>220</td></tr><tr><td>50</td><td>35</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td><td>200</td></tr><tr><td>52</td><td>34</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td><td>220</td></tr><tr><td>50</td><td>34</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td><td>220</td></tr><tr><td>51</td><td>35</td><td>Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt</td><td>Brush_teeth</td><td>220</td></tr></tbody></table></div>"]}}],"execution_count":15},{"cell_type":"code","source":["kmeans = KMeans(featuresCol=\"features\").setK(14).setSeed(1)\npipeline = Pipeline(stages=[vectorAssembler, kmeans])\nmodel = pipeline.fit(df_denormalized)\npredictions = model.transform(df_denormalized)\n\nevaluator = ClusteringEvaluator()\nsilhouette = evaluator.evaluate(predictions)\nprint(\"Kmeans with k = \" + str(14) + \": Silhouette with squared euclidean distance = \" + str(silhouette))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Kmeans with k = 14: Silhouette with squared euclidean distance = 0.5709023393004293\n</div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["We improved the score for k=14 clustering from 0.41 to 0.57 just by de-normalizing the x values!"],"metadata":{}}],"metadata":{"name":"Spark ML Clustering","notebookId":2993804106246670},"nbformat":4,"nbformat_minor":0}
