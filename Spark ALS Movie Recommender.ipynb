{"cells":[{"cell_type":"markdown","source":["# Movie Recommender using SparkML ALS\n\nThis notebook implements a recommender system using Apache Spark to recommend movies to a user.  More specifically, the sparkml library's Alternating Least Squares method is used to make predictions. The core algorithm behind ALS is Matrix Factorization with the added benefit of being able to run in parallel in a cluster.  \n\nWe will be using the popular MovieLens dataset. The movies.csv and ratings.csv files have been imported and stored as sql tables 'movies_small_csv' and 'ratings_small_csv' for easy access. A smaller version of the dataset has been used for demo purposes but the same code can run a much larger dataset using a bigger cluster."],"metadata":{}},{"cell_type":"code","source":["# load and cache data\n#sqlContext = SQLContext(sc)\nraw_ratings_df = sqlContext.sql(\"SELECT * FROM ratings_small_csv\")\nraw_movies_df = sqlContext.sql(\"SELECT * FROM movies_small_csv\")\n\n# drop the timestamp and genre columns since we won't be using them\nratings_df = raw_ratings_df.drop('timestamp')\nmovies_df = raw_movies_df.drop('genres')\n\n# cache the dataframes\nratings_df.cache()\nmovies_df.cache()\n\nraw_ratings_count = raw_ratings_df.count()\nratings_count = ratings_df.count()\nraw_movies_count = raw_movies_df.count()\nmovies_count = movies_df.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["print(raw_ratings_count)\nprint(raw_movies_count)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">100836\n9742\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["# verify some of the data\nassert ratings_df.is_cached\nassert movies_df.is_cached\nassert raw_ratings_count == ratings_count\nassert raw_movies_count == movies_count\nassert ratings_count ==  100836 # full dataset has 27,753,444\nassert movies_count == 9742 # full dataset has 58,098"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["display(movies_df.limit(5))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>movieId</th><th>title</th></tr></thead><tbody><tr><td>1</td><td>Toy Story (1995)</td></tr><tr><td>2</td><td>Jumanji (1995)</td></tr><tr><td>3</td><td>Grumpier Old Men (1995)</td></tr><tr><td>4</td><td>Waiting to Exhale (1995)</td></tr><tr><td>5</td><td>Father of the Bride Part II (1995)</td></tr></tbody></table></div>"]}}],"execution_count":5},{"cell_type":"code","source":["display(ratings_df.limit(5))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>userId</th><th>movieId</th><th>rating</th></tr></thead><tbody><tr><td>1</td><td>1</td><td>4.0</td></tr><tr><td>1</td><td>3</td><td>4.0</td></tr><tr><td>1</td><td>6</td><td>4.0</td></tr><tr><td>1</td><td>47</td><td>5.0</td></tr><tr><td>1</td><td>50</td><td>5.0</td></tr></tbody></table></div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["The image below shows the movie ratings matrix. The rows in the ratings matrix represents the ratings of each user and the columns represent each movie (item in general) for which we may have a rating. \n\nSince not all users have rated all movies, we do not know all of the entries in this matrix, therefore we need an algorithm like collaborative filtering to predict the missing ratings. In fact, in practice these rating matrices are often very sparse. In matrix factorization (MF) the idea is to approximate the ratings matrix by factorizing it as the product of two matrices: one that describes properties of each user (shown in green), and one that describes properties of each movie (shown in blue).\n\n<img alt=\"factorization\" src=\"http://spark-mooc.github.io/web-assets/images/matrix_factorization.png\" style=\"width: 885px\"/>\n<br clear=\"all\"/>\n\nInstead of directly decomposing the ratings matrix (e.g. by singular value decomposition) we use machine learning to to find two matrices such that the error for the available ratings is minimized. The [Alternating Least Squares][als] algorithm does this by first randomly filling the users matrix with values and then optimizing the value of the movies such that the error is minimized.  Then, it holds the movies matrix constant and optimizes the value of the user's matrix.  This alternation between which matrix to optimize is the reason for the \"alternating\" in the name.\n\nThe image on the right shows this alternating optimization. Using the a fixed set of user factors and the known ratings, we optimize for the best values for the movie factors.  Then we \"alternate\" and optimize for the best user factors using the latest fixed movie factors.\n\n[als]: https://en.wikiversity.org/wiki/Least-Squares_Method\n[mllib]: http://spark.apache.org/docs/latest/mllib-guide.html\n[collab]: https://en.wikipedia.org/?title=Collaborative_filtering\n[collab2]: http://recommender-systems.org/collaborative-filtering/"],"metadata":{}},{"cell_type":"markdown","source":["### Splitting the Dataset into Training, Validation and Test Sets\n\nWe can use the pySpark randomSplit() transformation. randomSplit() takes a set of splits and a seed and returns multiple DataFrames."],"metadata":{}},{"cell_type":"code","source":["# We will use a  60-20-20 split got for training, validation, and testing\nseed = 123\n(split_60_df, split_a_20_df, split_b_20_df) = ratings_df.randomSplit([0.6, 0.2, 0.2], seed=seed)\n\n# Let's cache these datasets for performance\ntraining_df = split_60_df.cache()\nvalidation_df = split_a_20_df.cache()\ntest_df = split_b_20_df.cache()\n\nprint('Training: {0}, validation: {1}, test: {2}\\n'.format(\n  training_df.count(), validation_df.count(), test_df.count())\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Training: 60541, validation: 20297, test: 19998\n\n</div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["### Tuning Parameters for Alternating Least Squares\n\nALS takes a training dataset (DataFrame) and a few model parameters. The most important parameter is the rank for matrix factorization (i.e. number of latent factors for users and items). In general, a lower rank will mean higher error on the training dataset, but a high rank may lead to [overfitting](https://en.wikipedia.org/wiki/Overfitting).  We will train models using the `training_df` dataset with ranks [10, 15, 20]. Regularization parameter is set to 0.1 (can also try other values but 0.1 works well for most models). Then we use the validation set to evaluate each model and keep the model with the best error. \n\nYou can read the documentation here: [ALS](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.recommendation.ALS)."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.recommendation import ALS\n\n# Initialize our ALS model object\nals = ALS()\n\n# Now we set the parameters for the method\nals.setMaxIter(5)\\\n   .setSeed(seed)\\\n   .setUserCol('userId') \\\n   .setItemCol('movieId') \\\n   .setRatingCol('rating') \\\n   .setRegParam(0.1)\n   #.setPredictionCol('prediction')\n\n# Now let's compute an evaluation metric for our test dataset\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Create an RMSE evaluator using the label and predicted columns\nreg_eval = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"rmse\")\n\ntolerance = 0.03\nranks = [10, 15, 20]\nerrors = []\nmodels = []\nmodel_index = 0\nmin_error = float('inf')\nbest_rank = -1\nbest_reg = -1\nbest_model_index = -1\nfor rank in ranks:\n  # Set the rank\n  als.setRank(rank)\n    \n  # Create the model with these parameters.\n  model = als.fit(training_df)\n  # Run the model to create a prediction. Predict against the validation_df.\n  predict_df = model.transform(validation_df)\n\n  # Remove NaN values from prediction (due to SPARK-14489)\n  #predicted_ratings_df = predict_df.filter(predict_df.prediction != float('nan'))\n  predicted_ratings_df = predict_df[predict_df.prediction != float('nan')]\n\n  # Run the previously created RMSE evaluator, \n  # reg_eval, on the predicted_ratings_df DataFrame\n  error = reg_eval.evaluate(predicted_ratings_df)\n  errors.append(error)\n  models.append(model)\n  print('For rank %s the RMSE is %s' % (rank, error))\n  if error < min_error:\n    min_error = error\n    best_rank = rank\n    best_model_index = model_index\n  model_index += 1\n\nals.setRank(best_rank)\nprint('The best model is model[%s] with rank %s and a RMSE on the validation set of %s' % (best_model_index, best_rank, errors[best_model_index]))\nmy_model = models[best_model_index]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">For rank 10 the RMSE is 0.9178427639543877\nFor rank 15 the RMSE is 0.9241693532548224\nFor rank 20 the RMSE is 0.9251354619304168\nThe best model is model[0] with rank 10 and a RMSE on the validation set of 0.9178427639543877\n</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["### Test the Model\nNow that we have picked a good rank for matrix factorization, let's test our model with the test dataset and see if we can get a comparable RMSE."],"metadata":{}},{"cell_type":"code","source":["# Use the trained model to get predictions. a new column 'prediction' will be added to the dataframe \npredict_df = my_model.transform(test_df)\n\n# Remove NaN values from prediction (due to SPARK-14489)\npredicted_test_df = predict_df.filter(predict_df.prediction != float('nan'))\n\n# Run the previously created RMSE evaluator, reg_eval, on the predicted_test_df DataFrame\ntest_RMSE = reg_eval.evaluate(predicted_test_df)\n\nprint('The model had a RMSE on the test set of {0}'.format(test_RMSE))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The model had a RMSE on the test set of 0.9199289611625379\n</div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["### Make Prediction for a New User\nLet's use our trained model to make some movie prediction for a new user. In order to do that we need to add some new ratings to ratings_df dataset. \n\nWe will first look at some popular movies to help us pick movies to rate."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import functions as F\n\n# From ratingsDF, create a movie_ids_with_avg_ratings_df that combines the two DataFrames\n# movie_ids_with_avg_ratings_df = ratings_df.groupBy('movieId').agg(F.count(ratings_df.rating).alias(\"count\"), F.avg(ratings_df.rating).alias(\"average\"))\n\nmovie_ids_with_avg_ratings_df = sqlContext.sql(\n  \"SELECT movieId, COUNT(rating) as count, AVG(rating) as average \\\n  FROM ratings_small_csv \\\n  group by movieId\"\n)\n\nprint('movie_ids_with_avg_ratings_df:')\nmovie_ids_with_avg_ratings_df.show(3, truncate=False)\n\nmovie_names_with_avg_ratings_df = movie_ids_with_avg_ratings_df.join(movies_df, 'movieId')\n\nprint('movie_names_with_avg_ratings_df:')\nmovie_names_with_avg_ratings_df.show(3, truncate=False)\n\n# Let's filter the dataframe to only those movies with at least 100 ratings\nmovies_with_100_ratings_or_more = movie_names_with_avg_ratings_df.filter(movie_names_with_avg_ratings_df['count'] >= 100)\nprint('Movies with highest ratings:')\ndisplay(movies_with_100_ratings_or_more.orderBy(movies_with_100_ratings_or_more['average'].desc()).limit(5))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>movieId</th><th>count</th><th>average</th><th>title</th></tr></thead><tbody><tr><td>318</td><td>317</td><td>4.429022082018927</td><td>Shawshank Redemption, The (1994)</td></tr><tr><td>858</td><td>192</td><td>4.2890625</td><td>Godfather, The (1972)</td></tr><tr><td>2959</td><td>218</td><td>4.272935779816514</td><td>Fight Club (1999)</td></tr><tr><td>1221</td><td>129</td><td>4.25968992248062</td><td>Godfather: Part II, The (1974)</td></tr><tr><td>48516</td><td>107</td><td>4.252336448598131</td><td>Departed, The (2006)</td></tr></tbody></table></div>"]}}],"execution_count":15},{"cell_type":"markdown","source":["The user ID 0 is unassigned, so we will use it for our ratings. We set the variable `my_user_ID` to 0 for the new user. Next, create a new DataFrame called `my_ratings_df`, with our ratings for at least 10 movie ratings. Each entry should be formatted as `(my_user_id, movieID, rating)`.  As in the original dataset, ratings should be between 1 and 5 (inclusive)."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import Row\nmy_user_id = 0\n\n# The format of each line is (my_user_id, movie ID, your rating)\n# For example, to give the movie \"Star Wars: Episode IV - A New Hope (1977)\" a five rating, we add the following line:\n#   (my_user_id, 260, 5),\nmy_rated_movies = [\n  (my_user_id, 231, 4.5), # Dumb and Dumber\n  (my_user_id, 296, 4.5), # Pulp Fiction\n  (my_user_id, 356, 5.0), # Forrest Gump\n  (my_user_id, 4993, 5.0), # Lord of the Rings: Fellowship of the Rings\n  (my_user_id, 1222, 3.5), # Full Metal Jacket\n  (my_user_id, 32, 3.5), # 12 Monkeys\n  (my_user_id, 4995, 5.0), # A Beautiful Mind\n  (my_user_id, 1206, 2.0), # A Clockwork Orange\n  (my_user_id, 1200, 4.0), # Aliens\n  (my_user_id, 1, 3.5), # Toy Story\n  (my_user_id, 589, 5.0), # Terminator 2: Judgement Day\n  (my_user_id, 7438, 5.0), # Kill Bill Vol.2\n  (my_user_id, 5445, 4.5), # Minority Report\n  (my_user_id, 34, 2.0), # Babe\n  (my_user_id, 44191, 2.0) # V for Vendetta\n]\n\nmy_ratings_df = sqlContext.createDataFrame(my_rated_movies, ['userId','movieId','rating'])\nprint('My movie ratings:')\ndisplay(my_ratings_df.limit(5))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>userId</th><th>movieId</th><th>rating</th></tr></thead><tbody><tr><td>0</td><td>231</td><td>4.5</td></tr><tr><td>0</td><td>296</td><td>4.5</td></tr><tr><td>0</td><td>356</td><td>5.0</td></tr><tr><td>0</td><td>4993</td><td>5.0</td></tr><tr><td>0</td><td>1222</td><td>3.5</td></tr></tbody></table></div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["### Adding Our Ratings to Training Dataset\nLet's now add our movie ratings to the training dataset so that our model can take our preferences into consideration."],"metadata":{}},{"cell_type":"code","source":["training_with_my_ratings_df = training_df.unionAll(my_ratings_df)\n\nprint ('The training dataset now has %s more entries than the original training dataset' %\n       (training_with_my_ratings_df.count() - training_df.count()))\nassert (training_with_my_ratings_df.count() - training_df.count()) == my_ratings_df.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The training dataset now has 15 more entries than the original training dataset\n</div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["### Train a Model with Your Ratings\n\nNow, train a model with your ratings added and the parameters used in the previous part."],"metadata":{}},{"cell_type":"code","source":["# Reset the parameters for the ALS object.\nals.setPredictionCol(\"prediction\")\\\n   .setMaxIter(5)\\\n   .setSeed(seed)\\\n   .setUserCol('userId') \\\n   .setItemCol('movieId') \\\n   .setRatingCol('rating') \\\n   .setRegParam(0.1) \\\n   .setRank(best_rank) #10\n\n# Create the model with these parameters.\nmy_ratings_model = als.fit(training_with_my_ratings_df)\n\n# Compute the prediction for this new model on the test set.\nmy_predict_df = my_ratings_model.transform(test_df)\n\n# Remove NaN values from prediction (due to SPARK-14489)\npredicted_test_my_ratings_df = my_predict_df.filter(my_predict_df.prediction != float('nan'))\n\n# Run the previously created RMSE evaluator, reg_eval, on the predicted_test_my_ratings_df DataFrame\ntest_RMSE_my_ratings = reg_eval.evaluate(predicted_test_my_ratings_df)\nprint('The model had a RMSE on the test set of {0}'.format(test_RMSE_my_ratings))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The model had a RMSE on the test set of 0.9232113162969088\n</div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["### Predict Ratings for New User\n\nNow let's predict what ratings we would give to the movies that we did not already provide ratings for.\n\nWe will first Filter out the movies you already rated manually. (Use the `my_rated_movie_ids` variable.) Put the results in a new `not_rated_df`. Then we use our new ratings model to make prediuctions on the unrated movies."],"metadata":{}},{"cell_type":"code","source":["# Create a list of my rated movie IDs\nmy_rated_movie_ids = [x[1] for x in my_rated_movies]\n\n# Filter out the movies we already rated.\nnot_rated_df = movies_df.filter(~ movies_df[\"movieId\"].isin(my_rated_movie_ids)) # \"NOT IN\"\ndisplay(not_rated_df.limit(5))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>movieId</th><th>title</th></tr></thead><tbody><tr><td>2</td><td>Jumanji (1995)</td></tr><tr><td>3</td><td>Grumpier Old Men (1995)</td></tr><tr><td>4</td><td>Waiting to Exhale (1995)</td></tr><tr><td>5</td><td>Father of the Bride Part II (1995)</td></tr><tr><td>6</td><td>Heat (1995)</td></tr></tbody></table></div>"]}}],"execution_count":23},{"cell_type":"code","source":["from pyspark.sql.functions import lit\n# Rename the \"ID\" column to be \"movieId\", and add a column with my_user_id as \"userId\".\nmy_unrated_movies_df = not_rated_df.withColumn(\"userId\", lit(my_user_id))\n\n# needed to add this line to avoid the exception\n# org.apache.spark.sql.AnalysisException: Detected implicit cartesian product \nspark.conf.set( \"spark.sql.crossJoin.enabled\" , \"true\" )\n\n# Use my_rating_model to predict ratings for the movies that we did not manually rate.\nraw_predicted_ratings_df = my_ratings_model.transform(my_unrated_movies_df)\n\npredicted_ratings_df = raw_predicted_ratings_df.filter(raw_predicted_ratings_df['prediction'] != float('nan'))\n\ndisplay(predicted_ratings_df.limit(5))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>movieId</th><th>title</th><th>userId</th><th>prediction</th></tr></thead><tbody><tr><td>471</td><td>Hudsucker Proxy, The (1994)</td><td>0</td><td>3.0968626</td></tr><tr><td>496</td><td>What Happened Was... (1994)</td><td>0</td><td>2.0698793</td></tr><tr><td>833</td><td>High School High (1996)</td><td>0</td><td>2.4305592</td></tr><tr><td>1088</td><td>Dirty Dancing (1987)</td><td>0</td><td>3.5631342</td></tr><tr><td>1238</td><td>Local Hero (1983)</td><td>0</td><td>2.903897</td></tr></tbody></table></div>"]}}],"execution_count":24},{"cell_type":"markdown","source":["Let's clean the predictions and give top 10 prediction with hightest scores."],"metadata":{}},{"cell_type":"code","source":["# First let's join our predicted_ratings_df with movie_names_with_avg_ratings_df to ontain the ratings counts for each movie.\npredicted_with_counts_df = predicted_ratings_df.join(movie_names_with_avg_ratings_df, [\"movieId\", \"title\"])\n\n# Then sort the resulting DataFrame (`predicted_with_counts_df`) by predicted rating (highest ratings first), and remove any ratings with a count of 50 or less.\npredicted_highest_rated_movies_df = predicted_with_counts_df.filter(predicted_with_counts_df['count'] >= 50)\n\n# Finally, print the top 10 movies that remain.\ntop_n = 10\nprint ('My %s highest rated movies as predicted (for movies with more than 50 reviews):' %top_n)\ndisplay(predicted_highest_rated_movies_df.orderBy(predicted_ratings_df['prediction'].desc()).take(top_n))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>movieId</th><th>title</th><th>userId</th><th>prediction</th><th>count</th><th>average</th></tr></thead><tbody><tr><td>76093</td><td>How to Train Your Dragon (2010)</td><td>0</td><td>4.8831048011779785</td><td>53</td><td>3.943396226415094</td></tr><tr><td>112852</td><td>Guardians of the Galaxy (2014)</td><td>0</td><td>4.844204902648926</td><td>59</td><td>4.0508474576271185</td></tr><tr><td>58559</td><td>Dark Knight, The (2008)</td><td>0</td><td>4.7725911140441895</td><td>149</td><td>4.238255033557047</td></tr><tr><td>91529</td><td>Dark Knight Rises, The (2012)</td><td>0</td><td>4.751042366027832</td><td>76</td><td>3.9934210526315788</td></tr><tr><td>54286</td><td>Bourne Ultimatum, The (2007)</td><td>0</td><td>4.729414939880371</td><td>81</td><td>3.697530864197531</td></tr><tr><td>116797</td><td>The Imitation Game (2014)</td><td>0</td><td>4.69578742980957</td><td>50</td><td>4.02</td></tr><tr><td>68954</td><td>Up (2009)</td><td>0</td><td>4.692851543426514</td><td>105</td><td>4.004761904761905</td></tr><tr><td>72998</td><td>Avatar (2009)</td><td>0</td><td>4.669037818908691</td><td>97</td><td>3.6030927835051547</td></tr><tr><td>89745</td><td>Avengers, The (2012)</td><td>0</td><td>4.662612438201904</td><td>69</td><td>3.869565217391304</td></tr><tr><td>2571</td><td>Matrix, The (1999)</td><td>0</td><td>4.645972728729248</td><td>278</td><td>4.192446043165468</td></tr></tbody></table></div>"]}}],"execution_count":26}],"metadata":{"name":"Spark ALS Movie Recommender","notebookId":2982915227324820},"nbformat":4,"nbformat_minor":0}
